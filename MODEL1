import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')
import folium
from datetime import datetime
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder

# 0. Initialize data
# Set file path
base_path = Path("D:/X-HEC/python for data science/pre/msdb-2024")

# Read data
train = pd.read_parquet(base_path / "train.parquet")
test = pd.read_parquet(base_path / "final_test.parquet")
external = pd.read_csv(base_path / "external_data.csv")

# View basic information of training data
print("Basic information of training data:")
print(train.info())
print("\nFirst few rows of data:")
print(train.head())
print("\nBasic statistical description:")
print(train.describe())

# Check for missing values
print("\nMissing value statistics:")
print(train.isnull().sum())

# 1. Time range analysis
print("Time Range:")
print(f"Start Date: {train['date'].min()}")
print(f"End Date: {train['date'].max()}")
print(f"Total Days: {(train['date'].max() - train['date'].min()).days}")

# 2. Counter station analysis
print("\nCounter Statistics:")
print(f"Number of Unique Counters: {train['counter_id'].nunique()}")
print(f"Number of Unique Sites: {train['site_id'].nunique()}")

# 3. Bike flow analysis
plt.figure(figsize=(10, 6))
sns.histplot(data=train, x='log_bike_count', bins=50)
plt.title('Log Bike Count Distribution')
plt.show()

# 4. Data volume per site
site_counts = train['site_id'].value_counts()
print("\nData Volume Statistics per Site:")
print(site_counts.describe())

# 5. Check for outliers
print("\nBasic Statistics of Bike Count:")
print(train['bike_count'].describe())

# 6. View Geographic Distribution using Folium
# Create a map centered around the average latitude and longitude
map_center = [train['latitude'].mean(), train['longitude'].mean()]
m = folium.Map(location=train[["latitude", "longitude"]].mean(axis=0), zoom_start=13)

# Iterate over each counter in the data and add a marker
for _, row in (
    train[["counter_name", "latitude", "longitude"]]
    .drop_duplicates("counter_name")
    .iterrows()
):
    folium.Marker(
        location=row[["latitude", "longitude"]].values.tolist(),  # Use latitude and longitude as location
        popup=row["counter_name"],  # Pop-up window showing counter name
        icon=folium.Icon(color='blue')  # Set the color of the marker
    ).add_to(m)

# Show map
m

# 7. Usage in different time periods (hours, weeks, months)
# Add time features
train['hour'] = train['date'].dt.hour
train['day_of_week'] = train['date'].dt.dayofweek
train['month'] = train['date'].dt.month

# Analyze traffic patterns by hour, day, and month
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Hourly distribution
train.groupby('hour')['bike_count'].mean().plot(kind='bar', ax=axes[0])
axes[0].set_title('Average Bike Count by Hour')

# Weekly distribution
train.groupby('day_of_week')['bike_count'].mean().plot(kind='bar', ax=axes[1])
axes[1].set_title('Average Bike Count by Day of Week')

# Monthly distribution
train.groupby('month')['bike_count'].mean().plot(kind='bar', ax=axes[2])
axes[2].set_title('Average Bike Count by Month')

plt.tight_layout()
plt.show()

# 8. Create detailed time features
def create_time_features(df):
    df['hour'] = df['date'].dt.hour
    df['day_of_week'] = df['date'].dt.dayofweek
    df['day_name'] = df['date'].dt.day_name()
    df['month'] = df['date'].dt.month
    df['month_name'] = df['date'].dt.month_name()
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['time_of_day'] = pd.cut(df['hour'], 
                              bins=[-1, 5, 11, 15, 19, 23],
                              labels=['Night', 'Morning', 'Noon', 'Afternoon', 'Evening'])
    return df

train = create_time_features(train)

# 9. Analyze time patterns
# plt.style.use('seaborn')

# 9.1 Hourly traffic analysis
plt.figure(figsize=(15, 6))
hourly_avg = train.groupby('hour')['bike_count'].agg(['mean', 'std']).reset_index()

plt.subplot(1, 2, 1)
plt.fill_between(hourly_avg['hour'], 
                hourly_avg['mean'] - hourly_avg['std'],
                hourly_avg['mean'] + hourly_avg['std'],
                alpha=0.2)
plt.plot(hourly_avg['hour'], hourly_avg['mean'], linewidth=2)
plt.title('Average Hourly Bike Count with Standard Deviation')
plt.xlabel('Hour of Day')
plt.ylabel('Bike Count')

# 9.2 Workday vs Weekend hourly patterns
plt.subplot(1, 2, 2)
workday = train[train['is_weekend'] == 0].groupby('hour')['bike_count'].mean()
weekend = train[train['is_weekend'] == 1].groupby('hour')['bike_count'].mean()

plt.plot(workday.index, workday.values, label='Workday')
plt.plot(weekend.index, weekend.values, label='Weekend')
plt.title('Workday vs Weekend Hourly Patterns')
plt.xlabel('Hour of Day')
plt.ylabel('Average Bike Count')
plt.legend()
plt.tight_layout()
plt.show()

# 10. Monthly and seasonal analysis
plt.figure(figsize=(15, 6))

# 10.1 Monthly trend
plt.subplot(1, 2, 1)
monthly_avg = train.groupby('month_name')['bike_count'].mean()
monthly_avg.plot(kind='bar')
plt.title('Average Monthly Bike Count')
plt.xticks(rotation=45)

# 10.2 Day of the week analysis
plt.subplot(1, 2, 2)
daily_avg = train.groupby('day_name')['bike_count'].mean()
daily_avg.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']).plot(kind='bar')
plt.title('Average Daily Bike Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 11. Heatmap: Hour vs Day of Week
plt.figure(figsize=(12, 8))
pivot_table = train.pivot_table(values='bike_count', 
                               index='hour',
                               columns='day_name',
                               aggfunc='mean')
pivot_table = pivot_table[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]
sns.heatmap(pivot_table, cmap='YlOrRd', center=pivot_table.mean().mean())
plt.title('Hour vs Day of Week Heatmap')
plt.show()

# 12. Basic statistics
print("\nTime period statistics:")
print(train.groupby('time_of_day')['bike_count'].agg(['mean', 'std', 'count']))

print("\nWorkday vs Weekend statistics:")
print(train.groupby('is_weekend')['bike_count'].agg(['mean', 'std', 'count']))

print("\nMonthly statistics:")
print(train.groupby('month_name')['bike_count'].agg(['mean', 'std', 'count']))

# 13. Improve time features
def create_advanced_features(df):
    # Copy the DataFrame to avoid modifying the original data
    df = df.copy()
    
    # Basic time-related features
    df['hour'] = df['date'].dt.hour
    df['day_of_week'] = df['date'].dt.dayofweek
    df['month'] = df['date'].dt.month
    df['day'] = df['date'].dt.day
    df['week_of_year'] = df['date'].dt.isocalendar().week
    
    # Derived time features
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['is_peak_hour'] = df['hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)
    df['is_night'] = df['hour'].isin(range(0, 5)).astype(int)
    
    # Seasonal features
    df['season'] = pd.cut(df['month'], 
                         bins=[0, 3, 6, 9, 12], 
                         labels=['Winter', 'Spring', 'Summer', 'Fall'])
    
    # Combined features
    df['is_workday_peak'] = ((df['is_weekend'] == 0) & 
                            (df['is_peak_hour'] == 1)).astype(int)
    
    return df

# Apply feature engineering
train_featured = create_advanced_features(train)

# Preview new features
print("New Feature Preview:")
print(train_featured[['hour', 'is_weekend', 'is_peak_hour', 'is_night', 
                     'season', 'is_workday_peak']].head())

# Validate feature effectiveness
print("\nCorrelation of features with bike count:")
correlations = train_featured[['bike_count', 'is_weekend', 'is_peak_hour', 
                             'is_night', 'is_workday_peak', 'hour', 
                             'day_of_week', 'month']].corr()['bike_count']
print(correlations)

# 14. Visualize the validation of new features
plt.figure(figsize=(15, 5))

# 14.1 Seasonal box plot
plt.subplot(1, 3, 1)
sns.boxplot(x='season', y='bike_count', data=train_featured)
plt.title('Bike Count by Season')
plt.xticks(rotation=45)

# 14.2 Peak vs Non-Peak Hours
plt.subplot(1, 3, 2)
sns.boxplot(x='is_peak_hour', y='bike_count', data=train_featured)
plt.title('Bike Count: Peak vs Non-Peak Hours')

# 14.3 Workday Peak vs Other Times
plt.subplot(1, 3, 3)
sns.boxplot(x='is_workday_peak', y='bike_count', data=train_featured)
plt.title('Bike Count: Workday Peak vs Others')

plt.tight_layout()
plt.show()

# 15. create lag features
def create_lag_features(df):
    df = df.copy()
    
    # Sort by counter ID and date
    df = df.sort_values(['counter_id', 'date'])
    
    # Create lag features
    df['previous_hour'] = df.groupby('counter_id')['bike_count'].shift(1)
    df['previous_day_same_hour'] = df.groupby(['counter_id', 'hour'])['bike_count'].shift(24)
    df['previous_week_same_hour'] = df.groupby(['counter_id', 'hour'])['bike_count'].shift(168)
    
    # Create rolling mean features
    df['rolling_mean_24h'] = df.groupby('counter_id')['bike_count'].rolling(window=24).mean().reset_index(0, drop=True)
    df['rolling_mean_7d'] = df.groupby('counter_id')['bike_count'].rolling(window=168).mean().reset_index(0, drop=True)
    
    return df

# 16. Apply lag features
train_featured = create_lag_features(train_featured)

# Preview new features
print("\nLag Feature Preview:")
print(train_featured[['bike_count', 'previous_hour', 'previous_day_same_hour', 
                     'previous_week_same_hour', 'rolling_mean_24h', 
                     'rolling_mean_7d']].head())

# Check the correlation of lag features
lag_correlations = train_featured[['bike_count', 'previous_hour', 
                                 'previous_day_same_hour', 'previous_week_same_hour',
                                 'rolling_mean_24h', 'rolling_mean_7d']].corr()['bike_count']
print("\nCorrelation of Lag Features with Bike Count:")
print(lag_correlations)

# 17. Visualize the effect of lag features
plt.figure(figsize=(15, 5))

# 17.1 Current vs Previous Hour
plt.subplot(1, 3, 1)
plt.scatter(train_featured['previous_hour'], train_featured['bike_count'], 
           alpha=0.1, s=1)
plt.title('Current vs Previous Hour')
plt.xlabel('Previous Hour Count')
plt.ylabel('Current Count')

# 17.2 Current vs Previous Day Same Hour
plt.subplot(1, 3, 2)
plt.scatter(train_featured['previous_day_same_hour'], train_featured['bike_count'], 
           alpha=0.1, s=1)
plt.title('Current vs Previous Day Same Hour')
plt.xlabel('Previous Day Count')
plt.ylabel('Current Count')

# 17.3 Current vs Rolling Mean
plt.subplot(1, 3, 3)
plt.scatter(train_featured['rolling_mean_24h'], train_featured['bike_count'], 
           alpha=0.1, s=1)
plt.title('Current vs 24h Rolling Mean')
plt.xlabel('24h Rolling Mean')
plt.ylabel('Current Count')

plt.tight_layout()
plt.show()

# 18. Generate multiple features
def prepare_features(df):
    df = df.copy()
    
    # 18.1 Basic time features
    df['hour'] = df['date'].dt.hour
    df['day_of_week'] = df['date'].dt.dayofweek
    df['month'] = df['date'].dt.month
    df['day'] = df['date'].dt.day
    
    # 18.2 Advanced time features
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['is_peak_hour'] = df['hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)
    df['is_night'] = df['hour'].isin(range(0, 5)).astype(int)
    df['is_workday_peak'] = ((df['is_weekend'] == 0) & 
                             (df['is_peak_hour'] == 1)).astype(int)
    
    # 18.3 Lag features
    df = df.sort_values(['counter_id', 'date'])
    for counter in df['counter_id'].unique():
        counter_mask = df['counter_id'] == counter
        
        # Hourly lag
        df.loc[counter_mask, 'previous_hour'] = df.loc[counter_mask, 'bike_count'].shift(1)
        df.loc[counter_mask, 'previous_2_hours'] = df.loc[counter_mask, 'bike_count'].shift(2)
        
        # Daily lag
        df.loc[counter_mask, 'previous_day_same_hour'] = df.loc[counter_mask, 'bike_count'].shift(24)
        
        # Weekly lag
        df.loc[counter_mask, 'previous_week_same_hour'] = df.loc[counter_mask, 'bike_count'].shift(168)
        
        # Rolling statistics
        df.loc[counter_mask, 'rolling_mean_24h'] = (
            df.loc[counter_mask, 'bike_count'].rolling(24).mean()
        )
        df.loc[counter_mask, 'rolling_std_24h'] = (
            df.loc[counter_mask, 'bike_count'].rolling(24).std()
        )
    
    # 18.4 Log transformation of the target variable
    # df['log_bike_count'] = np.log1p(df['bike_count'])
    df['log_bike_count'] = train['log_bike_count']
    
    return df

# 19. Training model construction
def train_model(df):
    # Prepare features
    feature_columns = [
        'hour', 'day_of_week', 'month', 'is_weekend', 'is_peak_hour',
        'is_night', 'is_workday_peak', 'previous_hour', 'previous_2_hours',
        'previous_day_same_hour', 'previous_week_same_hour',
        'rolling_mean_24h', 'rolling_std_24h'
    ]
    
    # Remove missing values
    df_clean = df.dropna(subset=feature_columns + ['log_bike_count'])
    
    # Prepare data
    X = df_clean[feature_columns]
    y = df_clean['log_bike_count']
    
    # Time series cross-validation
    tscv = TimeSeriesSplit(n_splits=5)
    
    # Model parameters
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'early_stopping_round': 50,  # Number of early stopping rounds
        'verbosity': -1  # Control log output
    }
    
    # Train model
    models = []
    scores = []
    
    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val)
        
        model = lgb.train(
            params,
            train_data,
            num_boost_round=1000,
            valid_sets=[val_data]
        )
        
        # Predict validation set
        val_pred = model.predict(X_val)
        score = np.sqrt(mean_squared_error(y_val, val_pred))
        scores.append(score)
        models.append(model)
    
    print(f"Average RMSE: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})")
    
    # Return the last model for prediction
    return models[-1]

# 20. Predict future weeks
def predict_future_weeks(model, last_data, weeks=3):
    hours_to_predict = weeks * 7 * 24  # Convert weeks to hours
    future_predictions = []
    current_data = last_data.copy()
    
    for hour in range(hours_to_predict):
        # Prepare prediction features
        pred_features = current_data.iloc[-1:][model.feature_name()]
        
        # Predict and convert back to actual values
        pred = np.expm1(model.predict(pred_features)[0])
        future_predictions.append(pred)
        
        # Create a new row
        new_row = current_data.iloc[-1:].copy()
        new_row['bike_count'] = pred
        
        # Update time features
        new_row['hour'] = (new_row['hour'] + 1) % 24
        if new_row['hour'].iloc[0] == 0:
            new_row['day_of_week'] = (new_row['day_of_week'] + 1) % 7
            new_row['day'] = new_row['day'] + 1
            if new_row['day'].iloc[0] > 28:  # Simplified month handling
                new_row['day'] = 1
                new_row['month'] = (new_row['month'] + 1) % 12
                if new_row['month'].iloc[0] == 0:
                    new_row['month'] = 12
        
        # Update time-related boolean features
        new_row['is_weekend'] = new_row['day_of_week'].isin([5, 6]).astype(int)
        new_row['is_peak_hour'] = new_row['hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)
        new_row['is_night'] = new_row['hour'].isin(range(0, 5)).astype(int)
        new_row['is_workday_peak'] = ((new_row['is_weekend'] == 0) & 
                                     (new_row['is_peak_hour'] == 1)).astype(int)
        
        # Update lag features
        new_row['previous_hour'] = current_data['bike_count'].iloc[-1]
        new_row['previous_2_hours'] = current_data['previous_hour'].iloc[-1]
        new_row['previous_day_same_hour'] = (
            current_data['bike_count'].iloc[-24] if len(current_data) >= 24 
            else current_data['bike_count'].mean()
        )
        new_row['previous_week_same_hour'] = (
            current_data['bike_count'].iloc[-168] if len(current_data) >= 168 
            else current_data['bike_count'].mean()
        )
        
        # Update rolling statistics
        recent_counts = pd.concat([current_data['bike_count'].tail(23), 
                                 pd.Series([pred])])
        new_row['rolling_mean_24h'] = recent_counts.mean()
        new_row['rolling_std_24h'] = recent_counts.std()
        
        # Add new row to current data
        current_data = pd.concat([current_data, new_row], ignore_index=True)
    
    return future_predictions

# 21. Predict and visualize
def plot_weekly_predictions(last_data, predictions, weeks=3):
    # Prepare actual data
    actual_week = last_data['bike_count'].values[-24*7:]  # Actual data for the last week
    
    # Create a chart
    fig, axes = plt.subplots(weeks, 1, figsize=(15, 5*weeks))
    fig.suptitle('Weekly Bike Traffic Predictions', fontsize=16)
    
    # Create a separate subplot for each week
    for week in range(weeks):
        start_idx = week * 24 * 7
        end_idx = (week + 1) * 24 * 7
        week_pred = predictions[start_idx:end_idx]
        
        if weeks > 1:
            ax = axes[week]
        else:
            ax = axes
            
        # Plot predicted data
        x = range(len(week_pred))
        ax.plot(x, week_pred, label=f'Week {week+1} Prediction', color='orange')
        
        # Also display actual data for the first week as a reference
        if week == 0:
            ax.plot(x[:len(actual_week)], actual_week, 
                   label='Last Week Actual', color='blue')
        
        # Add labels and grid
        ax.set_xlabel('Hours')
        ax.set_ylabel('Bike Count')
        ax.set_title(f'Week {week+1}')
        ax.grid(True, alpha=0.3)
        ax.legend()
        
        # Add date markers
        day_ticks = np.arange(0, 168, 24)
        ax.set_xticks(day_ticks)
        ax.set_xticklabels([f'Day {i+1}' for i in range(7)])
    
    plt.tight_layout()
    plt.show()

# 22. Complete forecast for the next three weeks
# Prepare data and train model
df_prepared = prepare_features(train)
model = train_model(df_prepared)

# Get the last data for predictions
last_data = df_prepared.dropna(subset=model.feature_name()).tail(168)  # Use the last week's data

# Predict for the next three weeks
weeks_to_predict = 3
predictions = predict_future_weeks(model, last_data, weeks=weeks_to_predict)

# Plot prediction results
plot_weekly_predictions(last_data, predictions, weeks=weeks_to_predict)

# Print weekly statistics
for week in range(weeks_to_predict):
    start_idx = week * 24 * 7
    end_idx = (week + 1) * 24 * 7
    week_pred = predictions[start_idx:end_idx]
    print(f"\nWeek {week+1} Statistics:")
    print(f"Average daily traffic: {np.mean(week_pred):.2f}")
    print(f"Peak traffic: {np.max(week_pred):.2f}")
    print(f"Minimum traffic: {np.min(week_pred):.2f}")

# 23 Complete the form
# 23.1 Check the training data
def prepare_features(df, is_training=True):
    df = df.copy()
    
    # Basic time features
    df['hour'] = df['date'].dt.hour
    df['day_of_week'] = df['date'].dt.dayofweek
    df['month'] = df['date'].dt.month
    df['day'] = df['date'].dt.day
    
    # Advanced time features
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['is_peak_hour'] = df['hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)
    df['is_night'] = df['hour'].isin(range(0, 5)).astype(int)
    df['is_workday_peak'] = ((df['is_weekend'] == 0) & 
                             (df['is_peak_hour'] == 1)).astype(int)
    
    if is_training:
        # Log transform the target
        df['log_bike_count'] = np.log1p(df['bike_count'])
        
        # Create lag features
        df = df.sort_values(['counter_id', 'date'])
        for counter in df['counter_id'].unique():
            counter_mask = df['counter_id'] == counter
            
            # Simple lag features
            df.loc[counter_mask, 'previous_hour'] = df.loc[counter_mask, 'bike_count'].shift(1)
            df.loc[counter_mask, 'previous_2_hours'] = df.loc[counter_mask, 'bike_count'].shift(2)
            
            # Daily and weekly patterns
            df.loc[counter_mask, 'previous_day_same_hour'] = df.loc[counter_mask, 'bike_count'].shift(24)
            df.loc[counter_mask, 'previous_week_same_hour'] = df.loc[counter_mask, 'bike_count'].shift(168)
            
            # Rolling statistics
            df.loc[counter_mask, 'rolling_mean_24h'] = (
                df.loc[counter_mask, 'bike_count'].rolling(24, min_periods=1).mean()
            )
            df.loc[counter_mask, 'rolling_std_24h'] = (
                df.loc[counter_mask, 'bike_count'].rolling(24, min_periods=1).std()
            )
    else:
        # For test data, initialize lag features with median values from training
        df['previous_hour'] = train['bike_count'].median()
        df['previous_2_hours'] = train['bike_count'].median()
        df['previous_day_same_hour'] = train['bike_count'].median()
        df['previous_week_same_hour'] = train['bike_count'].median()
        df['rolling_mean_24h'] = train['bike_count'].median()
        df['rolling_std_24h'] = train['bike_count'].std()
    
    return df

# 23.2 Modify the model training function
def train_model(df):
    # Prepare features
    feature_columns = [
        'hour', 'day_of_week', 'month', 'is_weekend', 'is_peak_hour',
        'is_night', 'is_workday_peak', 'previous_hour', 'previous_2_hours',
        'previous_day_same_hour', 'previous_week_same_hour',
        'rolling_mean_24h', 'rolling_std_24h'
    ]
    
    # Remove missing values
    df_clean = df.dropna(subset=feature_columns + ['log_bike_count'])
    
    # Prepare data
    X = df_clean[feature_columns]
    y = df_clean['log_bike_count']
    
    # Model parameters
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'verbosity': -1
    }
    
    # Train model
    train_data = lgb.Dataset(X, label=y)
    model = lgb.train(params, train_data, num_boost_round=1000)
    
    # Print feature importance
    importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': model.feature_importance()
    })
    print("\nFeature Importance:")
    print(importance.sort_values('importance', ascending=False))
    
    return model

# 23.3 Modify the prediction function
def predict_test_set(model, test_data):
    feature_columns = [
        'hour', 'day_of_week', 'month', 'is_weekend', 'is_peak_hour',
        'is_night', 'is_workday_peak', 'previous_hour', 'previous_2_hours',
        'previous_day_same_hour', 'previous_week_same_hour',
        'rolling_mean_24h', 'rolling_std_24h'
    ]
    
    # Prepare test features
    X_test = test_data[feature_columns]
    
    # Make predictions
    log_predictions = model.predict(X_test)
    
    # Convert back to actual values
    predictions = np.expm1(log_predictions)
    
    return predictions

# 23.4 Execute the forecasting process
# Prepare data
print("Preparing training data")
train_featured = prepare_features(train, is_training=True)

# Train model
print("Training model")
model = train_model(train_featured)

# Prepare test data
print("Preparing test data")
test_featured = prepare_features(test, is_training=False)

# Make predictions
print("Making predictions")
test_predictions = predict_test_set(model, test_featured)

# Create submission
submission = pd.DataFrame({
    'date': test['date'],
    'counter_id': test['counter_id'],
    'bike_count': np.round(test_predictions).astype(int)
})

# Save predictions
submission.to_csv('bike_count_predictions.csv', index=False)
print("\nPredictions saved to 'bike_count_predictions.csv'")

# Print statistics
print("\nTest Set Prediction Statistics:")
print(submission['bike_count'].describe())

# Visualize predictions
plt.figure(figsize=(15, 6))

# Distribution plot
plt.subplot(1, 2, 1)
sns.histplot(data=submission, x='bike_count', bins=50)
plt.title('Distribution of Predicted Bike Counts')
plt.xlabel('Predicted Bike Count')

# Time series plot for one counter
sample_counter = submission['counter_id'].iloc[0]
counter_data = submission[submission['counter_id'] == sample_counter].sort_values('date')

plt.subplot(1, 2, 2)
plt.plot(counter_data['date'], counter_data['bike_count'], 
         alpha=0.5, label='Raw Predictions')
plt.plot(counter_data['date'], 
         counter_data['bike_count'].rolling(24).mean(), 
         linewidth=2, label='24-hour Moving Average')
plt.title(f'Predictions Over Time for Counter {sample_counter}')
plt.xlabel('Date')
plt.ylabel('Predicted Bike Count')
plt.xticks(rotation=45)
plt.legend()

plt.tight_layout()
plt.show()

# 生成最终的CSV文件
# 将预测结果转换为log_bike_count格式
final_submission = pd.DataFrame({
    'id': range(len(test_predictions)),
    'log_bike_count': np.log1p(test_predictions)
})

# 确保列名完全匹配要求
final_submission.columns = ['id', 'log_bike_count']

# 保存为CSV文件，确保包含表头
submission_path = base_path / "submission.csv"
final_submission.to_csv(submission_path, index=False, header=True)
print(f"\nFinal submission file saved to: {submission_path}")

# 验证输出格式
print("\nFirst few rows of the submission file:")
print(final_submission.head())



# extra
def prepare_features(df, weather_df, is_training=True):
    df = df.copy()
    weather_df = weather_df.copy()
    
    # 确保日期格式一致
    df['date'] = pd.to_datetime(df['date'])
    weather_df['date'] = pd.to_datetime(weather_df['date'])
    
    # 基础时间特征
    df['hour'] = df['date'].dt.hour
    df['day'] = df['date'].dt.day
    df['month'] = df['date'].dt.month
    df['day_of_week'] = df['date'].dt.dayofweek
    df['week_of_year'] = df['date'].dt.isocalendar().week
    
    # 高级时间特征
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['is_peak_hour'] = df['hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)
    df['is_night'] = df['hour'].isin(range(0, 5)).astype(int)
    df['is_workday_peak'] = ((df['is_weekend'] == 0) & (df['is_peak_hour'] == 1)).astype(int)
    
    # 合并天气数据
    merged_df = pd.merge(df, weather_df, on='date', how='left')
    
    # 处理缺失值
    merged_df = merged_df.fillna(method='ffill')
    
    # 初始化滞后特征列（无论是训练还是测试）
    merged_df['previous_hour'] = 0
    merged_df['previous_day_same_hour'] = 0
    merged_df['previous_week_same_hour'] = 0
    merged_df['rolling_mean_24h'] = 0
    
    # 如果是训练数据，计算实际的滞后特征值
    if is_training:
        merged_df = merged_df.sort_values(['counter_id', 'date'])
        for counter in merged_df['counter_id'].unique():
            mask = merged_df['counter_id'] == counter
            merged_df.loc[mask, 'previous_hour'] = merged_df.loc[mask, 'bike_count'].shift(1)
            merged_df.loc[mask, 'previous_day_same_hour'] = merged_df.loc[mask, 'bike_count'].shift(24)
            merged_df.loc[mask, 'previous_week_same_hour'] = merged_df.loc[mask, 'bike_count'].shift(168)
            merged_df.loc[mask, 'rolling_mean_24h'] = merged_df.loc[mask, 'bike_count'].rolling(24).mean()
    
    # 选择特征
    features = [
        # 时间特征
        'hour', 'day', 'month', 'day_of_week', 'week_of_year',
        'is_weekend', 'is_peak_hour', 'is_night', 'is_workday_peak',
        
        # 天气特征
        't', 'td', 'u', 'ff', 'dd', 'vv', 'pmer', 'rr24',
        
        # 滞后特征
        'previous_hour', 'previous_day_same_hour', 
        'previous_week_same_hour', 'rolling_mean_24h'
    ]
    
    # 确保所有特征都存在
    available_features = [f for f in features if f in merged_df.columns]
    print(f"Using features: {available_features}")
    
    X = merged_df[available_features]
    X = X.fillna(X.mean())
    
    if is_training:
        y = merged_df['bike_count']
        return X, y
    else:
        return X

def train_and_predict(train, test, external):
    # 准备训练数据
    X, y = prepare_features(train, external, is_training=True)
    print(f"Training data shape: {X.shape}")
    
    # 训练模型
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
    model.fit(X, y)
    
    # 准备测试数据
    X_test = prepare_features(test, external, is_training=False)
    print(f"Test data shape: {X.shape}")
    
    # 检查特征列是否一致
    missing_features = set(X.columns) - set(X_test.columns)
    if missing_features:
        print(f"Warning: Missing features in test set: {missing_features}")
        
    # 确保测试数据具有与训练数据相同的列
    X_test = X_test[X.columns]
    
    # 预测
    predictions = model.predict(X_test)
    predictions = np.maximum(predictions, 0)  # 确保预测值不为负
    log_predictions = np.log1p(predictions)  # 计算log值
    
    # 创建提交文件
    submission_df = pd.DataFrame({
        'Id': range(len(predictions)),
        'log_bike_count': log_predictions
    })
    
    # 保存为CSV文件
    submission_df.to_csv('D:/X-HEC/python for data science/pre/msdb-2024/Submission0.csv', index=False)
    
    return submission_df

# 使用已导入的数据直接调用函数
submission = train_and_predict(train, test, external)
print("预测完成，结果已保存到 submission.csv")