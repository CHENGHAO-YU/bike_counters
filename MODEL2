import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import lightgbm as lgb
from datetime import datetime, timedelta
from pathlib import Path

base_path = Path("D:/X-HEC/python for data science/pre/msdb-2024")
train = pd.read_parquet(base_path / "train.parquet")
train.head()
test = pd.read_parquet(base_path / "final_test.parquet")
test.head()
external = pd.read_csv(base_path / "external.csv")
external.head()

import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
import lightgbm as lgb
from datetime import datetime
from sklearn.cluster import KMeans
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

def create_advanced_features(df, is_train=True):
    print("Creating features...")
    df = df.copy()
    
    # 统一时间戳格式
    df['date'] = pd.to_datetime(df['date']).astype('datetime64[ns]')
    
    # 基础时间特征
    df['hour'] = df['date'].dt.hour
    df['dayofweek'] = df['date'].dt.dayofweek
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    df['day'] = df['date'].dt.day
    df['week'] = df['date'].dt.isocalendar().week
    
    # 高级时间特征
    df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)
    df['is_holiday'] = df['dayofweek'].isin([5,6]).astype(int)
    df['is_rush_hour'] = df['hour'].isin([8,9,17,18]).astype(int)
    df['is_morning_rush'] = df['hour'].isin([8,9]).astype(int)
    df['is_evening_rush'] = df['hour'].isin([17,18]).astype(int)
    df['is_night'] = df['hour'].isin(range(22,6)).astype(int)
    df['is_working_hour'] = df['hour'].isin(range(9,18)).astype(int)
    
    # 周期性特征
    print("Creating cyclical features...")
    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)
    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)
    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)
    df['week_sin'] = np.sin(2 * np.pi * df['week']/52)
    df['week_cos'] = np.cos(2 * np.pi * df['week']/52)
    df['day_sin'] = np.sin(2 * np.pi * df['day']/31)
    df['day_cos'] = np.cos(2 * np.pi * df['day']/31)
    
    # 位置特征
    print("Creating location features...")
    coords = df[['latitude', 'longitude']].values
    df['location_cluster'] = KMeans(n_clusters=5, random_state=42).fit_predict(coords)
    
    # 组合特征
    df['rush_hour_weekend'] = df['is_rush_hour'] * df['is_weekend']
    df['hour_month'] = df['hour'] + df['month'] * 24
    
    if is_train:
        # 统计特征 (仅在训练集上创建)
        print("Creating statistical features...")
        for col in ['counter_id', 'hour', 'dayofweek', 'month']:
            df[f'{col}_mean_target'] = df.groupby(col)['log_bike_count'].transform('mean')
            df[f'{col}_std_target'] = df.groupby(col)['log_bike_count'].transform('std')
    
    return df

def process_weather_data(weather_df):
    print("Processing weather data...")
    weather_df = weather_df.copy()
    
    # 统一时间戳格式
    weather_df['date'] = pd.to_datetime(weather_df['date']).astype('datetime64[ns]')
    
    # 基础处理
    weather_features = ['date', 't', 'ff', 'u', 'pmer', 'rr1', 'rr3', 'rr6', 'rr12', 'rr24']
    weather_df = weather_df[weather_features]
    
    # 温度转换和特征工程
    weather_df['t'] = weather_df['t'] - 273.15
    weather_df['feels_like'] = weather_df['t'] - 0.2 * (1 - weather_df['u']/100)
    
    # 天气状况特征
    weather_df['is_rainy'] = (weather_df['rr1'] > 0).astype(int)
    weather_df['rain_intensity'] = pd.qcut(weather_df['rr1'].fillna(0), q=5, labels=False, duplicates='drop')
    
    # 天气变化特征
    for col in ['t', 'ff', 'u', 'pmer']:
        weather_df[f'{col}_change'] = weather_df[col].diff()
    
    return weather_df

def merge_weather_data(df, weather_df):
    print("Merging weather data...")
    # 确保日期格式一致
    df = df.copy()
    weather_df = weather_df.copy()
    
    df['date'] = pd.to_datetime(df['date']).astype('datetime64[ns]')
    weather_df['date'] = pd.to_datetime(weather_df['date']).astype('datetime64[ns]')
    
    # 排序
    df = df.sort_values('date')
    weather_df = weather_df.sort_values('date')
    
    # 合并
    merged_df = pd.merge_asof(
        df,
        weather_df,
        on='date',
        direction='nearest',
        tolerance=pd.Timedelta('1h')
    )
    
    return merged_df


# 修改特征列定义和模型训练部分
def get_feature_columns(train_df, test_df):
    """获取基础特征和统计特征"""
    # 基础特征
    base_features = ['hour', 'dayofweek', 'month', 'year', 'day', 'week',
                    'is_weekend', 'is_holiday', 'is_rush_hour', 'is_morning_rush',
                    'is_evening_rush', 'is_night', 'is_working_hour',
                    'hour_sin', 'hour_cos', 'month_sin', 'month_cos',
                    'week_sin', 'week_cos', 'day_sin', 'day_cos',
                    'location_cluster', 'rush_hour_weekend', 'hour_month',
                    't', 'ff', 'u', 'pmer', 'feels_like',
                    'is_rainy', 'rain_intensity', 'latitude', 'longitude']
    
    # 统计特征（仅在训练时使用）
    stat_features = [col for col in train_df.columns if '_target' in col]
    
    # 确保所有特征都在测试集中存在
    available_features = [col for col in base_features if col in test_df.columns]
    
    print(f"\nTotal base features: {len(available_features)}")
    print(f"Statistical features: {len(stat_features)}")
    
    return available_features, stat_features

def prepare_prediction_data(df, train_df, feature_cols):
    """准备预测数据，包括缺失值处理"""
    df = df.copy()
    
    # 处理缺失值
    for col in feature_cols:
        if col in df.columns and df[col].isnull().any():
            df[col] = df[col].fillna(train_df[col].mean())
    
    return df

def train_model(train_data, feature_cols, target_col):
    print("\nStarting model training...")
    
    # 基础模型参数
    base_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'verbose': -1,
        'n_jobs': -1,  # 使用所有CPU核心
    }
    
    # 不同参数配置
    param_sets = [
        # 配置1: 防止过拟合的保守配置
        {
            **base_params,
            'learning_rate': 0.01,
            'num_leaves': 31,
            'max_depth': 6,
            'min_data_in_leaf': 30,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'lambda_l1': 0.5,
            'lambda_l2': 0.5
        },
        # 配置2: 较为激进的配置
        {
            **base_params,
            'learning_rate': 0.03,
            'num_leaves': 50,
            'max_depth': 8,
            'min_data_in_leaf': 20,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.9,
            'bagging_freq': 3,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1
        },
        # 配置3: 平衡配置
        {
            **base_params,
            'learning_rate': 0.02,
            'num_leaves': 40,
            'max_depth': 7,
            'min_data_in_leaf': 25,
            'feature_fraction': 0.85,
            'bagging_fraction': 0.85,
            'bagging_freq': 4,
            'lambda_l1': 0.3,
            'lambda_l2': 0.3
        }
    ]
    
    # 使用时间序列交叉验证
    tscv = TimeSeriesSplit(n_splits=5)
    all_models = []
    all_scores_rmse = []
    all_scores_r2 = []
    feature_importance_df = pd.DataFrame()
    
    # 用于存储每个配置的平均性能
    config_performances = []
    
    # 对每个参数配置进行训练
    for config_idx, params in enumerate(param_sets):
        print(f"\nTraining with parameter configuration {config_idx + 1}")
        models = []
        scores_rmse = []
        scores_r2 = []
        
        # 交叉验证
        for fold, (train_idx, val_idx) in enumerate(tqdm(tscv.split(train_data), 
                                                       desc=f"Config {config_idx + 1} folds",
                                                       total=5)):
            X_train = train_data.iloc[train_idx][feature_cols]
            y_train = train_data.iloc[train_idx][target_col]
            X_val = train_data.iloc[val_idx][feature_cols]
            y_val = train_data.iloc[val_idx][target_col]
            
            train_set = lgb.Dataset(X_train, y_train)
            val_set = lgb.Dataset(X_val, y_val, reference=train_set)
            
            # 使用早停
            callbacks = [
                lgb.early_stopping(stopping_rounds=50),
                lgb.log_evaluation(period=100)
            ]
            
            model = lgb.train(
                params,
                train_set,
                num_boost_round=3000,  # 增加最大迭代次数
                valid_sets=[val_set],
                callbacks=callbacks
            )
            
            # 验证集预测
            val_pred = model.predict(X_val)
            rmse = np.sqrt(mean_squared_error(y_val, val_pred))
            r2 = r2_score(y_val, val_pred)
            
            scores_rmse.append(rmse)
            scores_r2.append(r2)
            models.append(model)
            
            # 记录特征重要性
            fold_importance = pd.DataFrame({
                'feature': feature_cols,
                'importance': model.feature_importance(),
                'fold': fold,
                'config': config_idx
            })
            feature_importance_df = pd.concat([feature_importance_df, fold_importance], axis=0)
            
            print(f"Fold {fold + 1} - RMSE: {rmse:.4f}, R2: {r2:.4f}")
        
        # 计算此配置的平均性能
        avg_rmse = np.mean(scores_rmse)
        avg_r2 = np.mean(scores_r2)
        config_performances.append({
            'config': config_idx,
            'avg_rmse': avg_rmse,
            'avg_r2': avg_r2,
            'models': models
        })
        
        print(f"\nConfiguration {config_idx + 1} Performance:")
        print(f"Average RMSE: {avg_rmse:.4f} (±{np.std(scores_rmse):.4f})")
        print(f"Average R2: {avg_r2:.4f} (±{np.std(scores_r2):.4f})")
    
    # 选择最佳配置
    best_config = min(config_performances, key=lambda x: x['avg_rmse'])
    print(f"\nBest configuration: {best_config['config'] + 1}")
    print(f"Best RMSE: {best_config['avg_rmse']:.4f}")
    print(f"Best R2: {best_config['avg_r2']:.4f}")
    
    # 分析特征重要性
    print("\nAnalyzing feature importance...")
    mean_importance = feature_importance_df.groupby('feature')['importance'].mean()
    std_importance = feature_importance_df.groupby('feature')['importance'].std()
    
    feature_importance = pd.DataFrame({
        'feature': mean_importance.index,
        'importance_mean': mean_importance.values,
        'importance_std': std_importance.values
    }).sort_values('importance_mean', ascending=False)
    
    print("\nTop 15 most important features:")
    print(feature_importance.head(15))
    
    # 返回最佳配置的模型和特征重要性
    return best_config['models'], feature_importance


# [前面的代码保持不变直到数据清理部分]

# 主流程
print("Starting data preprocessing...")
train = create_advanced_features(train, is_train=True)
test = create_advanced_features(test, is_train=False)

# 处理天气数据
external_processed = process_weather_data(external)

print("Merging data with weather information...")
train = merge_weather_data(train, external_processed)
test = merge_weather_data(test, external_processed)

# 获取特征列
base_features, stat_features = get_feature_columns(train, test)

# 为测试集创建统计特征
print("\nCreating statistical features for test set...")
for col in ['counter_id', 'hour', 'dayofweek', 'month']:
    # 计算训练集中每个类别的平均值和标准差
    means = train.groupby(col)['log_bike_count'].mean()
    stds = train.groupby(col)['log_bike_count'].std()
    
    # 创建映射字典
    mean_dict = means.to_dict()
    std_dict = stds.to_dict()
    
    # 使用map方法创建新的统计特征
    test[f'{col}_mean_target'] = test[col].apply(lambda x: mean_dict.get(x, means.mean()))
    test[f'{col}_std_target'] = test[col].apply(lambda x: std_dict.get(x, stds.mean()))

# 确保所有特征都存在
all_features = base_features + stat_features
print(f"\nTotal features: {len(all_features)}")
print("Features:", all_features)

# 数据类型转换和清理
print("Converting data types and cleaning...")
numeric_features = []
for col in all_features:
    try:
        if col in train.columns:
            train[col] = pd.to_numeric(train[col], errors='coerce')
            numeric_features.append(col)
        if col in test.columns:
            test[col] = pd.to_numeric(test[col], errors='coerce')
    except (ValueError, TypeError):
        print(f"Warning: Could not convert {col} to numeric")

print(f"\nNumeric features: {len(numeric_features)} out of {len(all_features)}")

# 处理无穷值
train = train.replace([np.inf, -np.inf], np.nan)
test = test.replace([np.inf, -np.inf], np.nan)

# 分别处理数值型和分类型特征的缺失值
print("\nHandling missing values...")
for col in all_features:
    if col in numeric_features:
        # 对数值型特征使用均值填充
        if train[col].isnull().any():
            fill_value = train[col].mean()
            train[col] = train[col].fillna(fill_value)
        if test[col].isnull().any():
            fill_value = train[col].mean() if col in train.columns else test[col].mean()
            test[col] = test[col].fillna(fill_value)
    else:
        # 对分类型特征使用众数填充
        if train[col].isnull().any():
            fill_value = train[col].mode()[0]
            train[col] = train[col].fillna(fill_value)
        if test[col].isnull().any():
            fill_value = train[col].mode()[0] if col in train.columns else test[col].mode()[0]
            test[col] = test[col].fillna(fill_value)

# 验证特征匹配
print("\nVerifying features...")
missing_train = [col for col in all_features if col not in train.columns]
missing_test = [col for col in all_features if col not in test.columns]

if missing_train or missing_test:
    print("Missing features in train:", missing_train)
    print("Missing features in test:", missing_test)
    raise ValueError("Missing features detected")

# 确保所有用于模型的特征都是浮点型
print("\nFinal data type conversion...")
for col in all_features:
    train[col] = train[col].astype(float)
    test[col] = test[col].astype(float)

# 最终的数据验证
print("\nValidating final data...")
print("Training data shape:", train[all_features].shape)
print("Test data shape:", test[all_features].shape)

# 检查是否还有缺失值
train_null = train[all_features].isnull().sum()
test_null = test[all_features].isnull().sum()

if train_null.any() or test_null.any():
    print("\nWarning: There are still null values in the data:")
    print("\nTraining null values:\n", train_null[train_null > 0])
    print("\nTest null values:\n", test_null[test_null > 0])
    
    # 最后的缺失值处理
    train[all_features] = train[all_features].fillna(0)
    test[all_features] = test[all_features].fillna(0)

# 训练模型
print("\nTraining models...")
models, feature_importance = train_model(train, all_features, 'log_bike_count')

# 预测
print("\nMaking predictions...")
predictions = np.zeros(len(test))
for i, model in enumerate(tqdm(models, desc="Generating predictions")):
    test_features = test[all_features].copy()
    predictions += model.predict(test_features)
predictions /= len(models)

# 后处理预测结果
predictions = np.clip(predictions, 0, None)  # 确保预测值不为负

# 创建提交文件
submission = pd.DataFrame({
    'date': test['date'],
    'counter_id': test['counter_id'],
    'log_bike_count': predictions
})

print("\nSaving predictions...")
submission.to_csv('submission.csv', index=False)
print("Done!")

# 输出预测统计信息
print("\nPrediction statistics:")
print(pd.Series(predictions).describe())

# 保存特征重要性
feature_importance.sort_values(ascending=False).to_csv('feature_importance.csv')
print("\nFeature importance saved to 'feature_importance.csv'")